{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50]\n",
      "Number of classes: 51\n",
      "Step 50: Train Loss 0.2786 FER = 24.03%, Val Loss = 2.9559\n",
      "Step 100: Train Loss 0.1736 FER = 27.00%, Val Loss = 4.5005\n",
      "Step 150: Train Loss 0.1455 FER = 3.53%, Val Loss = 0.1251\n",
      "Step 200: Train Loss 0.1125 FER = 6.60%, Val Loss = 0.8809\n",
      "Step 250: Train Loss 0.4239 FER = 7.21%, Val Loss = 0.8020\n",
      "Step 300: Train Loss 0.0567 FER = 16.92%, Val Loss = 1.3624\n",
      "Early stopping triggered\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   1%|‚ñè         | 18/1250 [00:00<00:12, 96.56batch/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'output/decoder_classifier/weights.pth'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort Song --> Train TweetyBERT --> HDBSCAN --> Manual Overview --> Merge / Identify Noise Clusters --> Train Classifier --> Analyze \n",
    "\n",
    "from scripts.npz_file_train_test_split import split_npz_into_chunks\n",
    "from scripts.label_merger import read_label_file, process_labels\n",
    "from src.tweety_bert.linear_probe import LinearProbeModel, LinearProbeTrainer, ModelEvaluator\n",
    "from src.tweety_bert.utils import load_model\n",
    "from src.tweety_bert.data_class import SongDataSet_Image, CollateFunction\n",
    "from src.syllable_classifier.classify_songs import Inference \n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import json \n",
    "import os\n",
    "import shutil\n",
    "\n",
    "input_file = 'merge_combine.txt'  # Path to the input file\n",
    "npz_file = 'files/labels_HDBSCAN_Classification.npz'  # Path to the npz file\n",
    "\n",
    "combine, noise = read_label_file(input_file)\n",
    "num_classes = process_labels(npz_file, combine, noise)\n",
    "\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "# load parameters.json\n",
    "with open('parameters.json', 'r') as f:\n",
    "    parameters = json.load(f)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load TweetyBERT model\n",
    "weights_path = \"/media/george-vengrovski/disk1/song_analysis_pipeline_testing_delete_when_works/temp/testingpretrain_run/5377_test_run/saved_weights/model_step_19999.pth\"\n",
    "config_path = \"/media/george-vengrovski/disk1/song_analysis_pipeline_testing_delete_when_works/temp/testingpretrain_run/5377_test_run/config.json\"\n",
    "tweety_bert_model = load_model(config_path, weights_path)\n",
    "\n",
    "# split into train / test \n",
    "train_folder = \"/media/george-vengrovski/disk1/song_analysis_pipeline_testing_delete_when_works/temp/linear_classifier_train\"\n",
    "test_folder = \"/media/george-vengrovski/disk1/song_analysis_pipeline_testing_delete_when_works/temp/linear_classifier_test\"\n",
    "split_npz_into_chunks(npz_file, chunk_size=1000, train_folder=train_folder, test_folder=test_folder)\n",
    "\n",
    "# Train Non-Linear Classifier\n",
    "train_dataset = SongDataSet_Image(train_folder, num_classes=num_classes, decoder=True)\n",
    "val_dataset = SongDataSet_Image(test_folder, num_classes=num_classes, decoder=True)\n",
    "collate_fn = CollateFunction(segment_length=1000)  # Adjust the segment length if needed\n",
    "train_loader = DataLoader(train_dataset, batch_size=24, shuffle=False, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=24, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Initialize and train classifier model, the num classes is a hack and needs to be fixed later on by removing one hot encodings \n",
    "classifier_model = LinearProbeModel(num_classes=num_classes, model_type=\"neural_net\", model=tweety_bert_model,\n",
    "                                    freeze_layers=True, layer_num=-2, layer_id=\"attention_output\", classifier_dims=196)\n",
    "\n",
    "classifier_model = classifier_model.to(device)\n",
    "trainer = LinearProbeTrainer(model=classifier_model, train_loader=train_loader, test_loader=val_loader,\n",
    "                                device=device, lr=1e-4, plotting=False, batches_per_eval=50, desired_total_batches=1e4, patience=4)\n",
    "trainer.train()\n",
    "\n",
    "eval_dataset = SongDataSet_Image(test_folder, num_classes=num_classes, infinite_loader=False, decoder=True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Evaluate the trained model\n",
    "evaluator = ModelEvaluator(model=classifier_model, test_loader=eval_loader, num_classes=num_classes,\n",
    "                            device='cuda:0', filter_unseen_classes=True, train_dir=train_folder)\n",
    "                            \n",
    "class_frame_error_rates, total_frame_error_rate = evaluator.validate_model_multiple_passes(num_passes=1, max_batches=1250)\n",
    "\n",
    "# Use the name of the current cross-validation directory for the results folder\n",
    "results_folder_name = os.path.basename(test_folder)\n",
    "\n",
    "# Save the evaluation results\n",
    "results_dir = os.path.join(\"/home/george-vengrovski/Documents/projects/song_analysis_pipeline/temp\", results_folder_name)  # Modified to save into the relative path /results/{cv_dirs}\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "evaluator.save_results(class_frame_error_rates, total_frame_error_rate, results_dir)\n",
    "\n",
    "# Save Linear Decoder + Its Parameters \n",
    "save_path = os.path.join(\"output/decoder_classifier\", \"linear_decoder.pth\")\n",
    "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "torch.save(classifier_model.state_dict(), save_path)\n",
    "\n",
    "# save a copy of the tweetybert config json and weights in the same folder \n",
    "shutil.copy(config_path, os.path.join(\"output/decoder_classifier\", \"config.json\"))\n",
    "shutil.copy(weights_path, os.path.join(\"output/decoder_classifier\", \"weights.pth\"))\n",
    "\n",
    "# run inference, output csv to the /output folder\n",
    "# if sort songs is false, than we assume that the files that have been provided have already been sorted \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Use it to analyze whole dataset \n",
    "# import shutil\n",
    "\n",
    "# # delete temp folder contents\n",
    "# temp_folder_path = parameters[\"temp_path\"]\n",
    "# shutil.rmtree(temp_folder_path)\n",
    "# os.makedirs(temp_folder_path, exist_ok=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "WavtoSpec.__init__() missing 2 required positional arguments: 'src_dir' and 'dst_dir'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m Inference(input_path\u001b[38;5;241m=\u001b[39mparameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_path\u001b[39m\u001b[38;5;124m\"\u001b[39m], output_path\u001b[38;5;241m=\u001b[39moutput_path, plot_spec_results\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, model\u001b[38;5;241m=\u001b[39mclassifier_model, sorted_songs_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/media/george-vengrovski/disk1/song_analysis_pipeline_testing_delete_when_works/temp/database.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m.5\u001b[39m, min_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m, pad_song\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# else:\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#     pass\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m model\u001b[38;5;241m.\u001b[39mclassify_all_songs()\n",
      "File \u001b[0;32m~/Documents/projects/song_analysis_pipeline/src/syllable_classifier/classify_songs.py:92\u001b[0m, in \u001b[0;36mInference.classify_all_songs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclassify_all_songs\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 92\u001b[0m     wav_to_spec \u001b[38;5;241m=\u001b[39m WavtoSpec()\n\u001b[1;32m     93\u001b[0m     rows_to_add \u001b[38;5;241m=\u001b[39m []  \u001b[38;5;66;03m# Initialize an empty list to collect rows\u001b[39;00m\n\u001b[1;32m     95\u001b[0m     total_songs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: WavtoSpec.__init__() missing 2 required positional arguments: 'src_dir' and 'dst_dir'"
     ]
    }
   ],
   "source": [
    "output_path = os.path.join(parameters[\"output_path\"], \"syllable_annotations_output\")\n",
    "# if parameters[\"sort_songs\"] == True:\n",
    "model = Inference(input_path=parameters[\"input_path\"], output_path=output_path, plot_spec_results=True, model=classifier_model, sorted_songs_path=\"/media/george-vengrovski/disk1/song_analysis_pipeline_testing_delete_when_works/temp/database.csv\", threshold=.5, min_length=500, pad_song=50)\n",
    "# else:\n",
    "#     pass\n",
    "model.classify_all_songs()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tweetybert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
